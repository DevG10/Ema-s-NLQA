training cs324 link search menu expand document cs324 home calendar lecture introduction capability harm harm ii data security legality modeling training parallelism scaling law selective architecture adaptation environmental impact paper review paper discussion project site us doc documentation theme jekyll lecture training newcommandsvmathcalv newcommandsomathcalo newcommandsdmathcald newcommandsnmathcaln newcommandrmathbbr newcommandemathbbe newcommandxx1l newcommandtxtilde x1l newcommandnl1textsf1 newcommandsoftmaxtextsoftmax newcommandtransformerblocktexttransformerblock newcommandembedtokenwithpositiontextembedtokenwithposition newcommandsentenceembeddingtextsentenceembedding newcommandberttextbert newcommandmasknlmask newcommandsepnlsep newcommandsentenceembeddingtextsentenceembedding newcommandberttextbert newcommandmasknlmask newcommandsepnlsep newcommandclsnlcls newcommandgenerate1stackrel1rightsquigarrow newcommandembedstackrelphirightarrow last lecture talked model architecture large language model eg transformer lecture discus train large language model objective function optimization algorithm objective function consider objective function three type language model decoderonly eg gpt3 compute unidirectional contextual embeddings generate one token time encoderonly eg bert compute bidirectional contextual embeddings encoderdecoder eg t5 encode input decode output use model map token sequence contextual embeddings eg lstms transformer phi svl rd time l nlthe eg t5 encode input decode output use model map token sequence contextual embeddings eg lstms transformer phi svl rd time l nlthe nlmouse nlate nlthe nlcheese embed leftbinom101 binom01 binom11 binom101 binom01 right decoderonly model recall autoregressive language model defines conditional distribution pxi mid x1i1 define follows map x1i1 contextual embeddings phix1i1 apply embedding matrix e rv time obtain score token e phix1i1i1 exponentiate normalize produce distribution xi succinctly pxi1 mid x1i softmaxe phix1ii maximum likelihood let theta parameter large language model let sd training data consisting set sequence follow maximum maximum likelihood let theta parameter large language model let sd training data consisting set sequence follow maximum likelihood principle define following negative loglikelihood objective function sotheta sumx sd log pthetax sumx sd sumi1l log pthetaxi mid x1i1 there say efficiently optimize function thats objective encoderonly model unidirectional bidirectional decoderonly model trained using maximum likelihood also produce unidirectional contextual embeddings provide stronger bidirectional contextual embeddings given dont need generate bert first present bert objective function contains two term masked language modeling next sentence prediction take example sequence natural language inference predict entailment first present bert objective function contains two term masked language modeling next sentence prediction take example sequence natural language inference predict entailment contradiction neutral x cl nlall nlanimals nlbreathe sep nlcats nlbreathe two special token cl contains embedding used drive classification task sep used tell model first eg premise versus second sequence eg hypothesis using notation previous lecture bert model defined bertx transformerblock24embedtokenwithpositionx sentenceembeddingx rd time l sentenceembeddingx return one 2 vector depending sequence ea rd token left sep eb rd token right sep bertlarge ntextheads 16 attention head dtextmodel 1024 dimensional model rd token left sep eb rd token right sep bertlarge ntextheads 16 attention head dtextmodel 1024 dimensional model resulting 355m parameter masked language modeling basic idea masked language model train prediction problem nlthe mask nlate mask nlcheese rightarrow nlthe nlmouse nlate nlthe nlcheese generally think similar denoising autoencoder map noisy incomplete version tx try reconstruct original x tx rightarrow x model first define model distribution take tx predicts token independently given contextual embedding pxi mid tx softmaxe phitxi masking function define stochastic noising function atx mid x underbracextextoriginal stackrelarightarrow pxi mid tx softmaxe phitxi masking function define stochastic noising function atx mid x underbracextextoriginal stackrelarightarrow underbracetxtextnoised here defined let subset 1 dot l random 15 token position probability 08 set tilde xi leftarrow mask probability 01 set tilde xi leftarrow xi probability 01 set tilde xi leftarrow textrandom word sv reducing distribution shift always replace chosen token mask training every input bert would see sequence mask test time would feed sentence mask resulting distribution shift heuristic fix replace real word 20 time next sentence prediction recall bert trained pair sentence concatenated goal next distribution shift heuristic fix replace real word 20 time next sentence prediction recall bert trained pair sentence concatenated goal next sentence prediction predict whether second sentence follows first cl nlthe nlmouse nlate nlthe nlcheese sep nlit nlwas nlfull rightarrow 1 cl nlthe nlmouse nlate nlthe nlcheese sep nlhello nlworld rightarrow 0 use embedding cl token make binary classification decision dataset let sd set example x c constructed follows let sentence corpus probability 05 let b next sentence probability 05 let b random sentence corpus let x cl sep b let c denote whether b next sentence objective bert objective probability 05 let b random sentence corpus let x cl sep b let c denote whether b next sentence objective bert objective sotheta sumxc sd underbraceei tx sim acdot mid x ileftsumi log pthetatilde xi mid xrighttextmasked language modeling underbracelog pc mid phix1textnext sentence prediction talk training later quick note bert bert along elmo ulmfit showed one uniform architecture transformer could used many multiple classification task bert really transformed nlp community pretraining finetuning mindset bert showed importance deeply bidirectional contextual embeddings although possible model size finetuning strategy make ptuning roberta make following change bert removed next sentence contextual embeddings although possible model size finetuning strategy make ptuning roberta make following change bert removed next sentence prediction objective found didnt help trained data 16gb text rightarrow 160gb text trained longer roberta improved accuracy significantly bert various benchmark eg squad 818 894 encoderdecoder model example task tabletotext generation nlname nl nlclowns nl nleattype nl nlcoffee nlshop rightarrow nlclowns nlis nla nlcoffee nlshop recall encoderdecoder model eg bart t5 encode input bidirectionally like bert decode output autoregressively like gpt2 bart bidirectional autoregressive transformer bart lewis et al 2019 transformerbased encoderdecoder model encoder architecture roberta 12 layer hidden dimension 1024 trained autoregressive transformer bart lewis et al 2019 transformerbased encoderdecoder model encoder architecture roberta 12 layer hidden dimension 1024 trained data roberta 160gb text bart considers following transformation atx mid x based bertscaled experiment decided following transformation final model mask 30 token document permute sentence demonstrated strong result classification generation task using finetuning t5 texttotext transfer transformer t5 raffel et al 2020 another transformerbased encoderdecoder model task given span text split random point input output nlthe nlmouse rightarrow nlate nlthe nlcheese paper experimented many different unsupervised objective found iid noise replace span worked well though many objective similar also cast paper experimented many different unsupervised objective found iid noise replace span worked well though many objective similar also cast classical nlp task uniform framework texttotext task note difference approach classification task bert used embedding cl token predict t5 gpt2 gpt3 etc model generate cast classification task natural language space note paper thorough study many aspect entire pipeline dataset model size training objective etc based insight trained 11b parameter model optimization algorithm turn attention optimize objective simplicity let take autogressive language modeling sotheta sumx sd log pthetax stochastic gradient descent sgd first cut stochastic gradient descent minibatches initialize parameter modeling sotheta sumx sd log pthetax stochastic gradient descent sgd first cut stochastic gradient descent minibatches initialize parameter theta0 repeat sample minibatch bt subset sd perform gradient step thetat leftarrow thetat1 eta frac1bt sumx bt nablatheta log pthetax key concern optimization want theta converge quickly good solution want optimization numerically stable want memory efficient especially large model often odds eg fast convergence cutting memory lowprecision produce le stable training several level approach optimization classic optimization secondorder method constrained optimization etc machine learning stochastic method implicit regularization early stopping deep learning initialization optimization classic optimization secondorder method constrained optimization etc machine learning stochastic method implicit regularization early stopping deep learning initialization normalization change model architecture large language model stability issue weird learning rate intuition eg secondorder method still useful many unique challenge need overcome large language model training work unfortunately much fairly adhoc poorly understood adam adaptive moment estimation adam incorporates two idea use momentum keep moving direction adaptive different step size dimension theta inspiration secondorder method initialize parameter theta0 initialize moment m0 v0 leftarrow 0 repeat sample minibatch bt subset sd update parameter follows updating parameter compute gradient gt parameter theta0 initialize moment m0 v0 leftarrow 0 repeat sample minibatch bt subset sd update parameter follows updating parameter compute gradient gt leftarrow frac1bt sumx bt nablatheta log pthetax update first secondorder moment mt leftarrow beta1 mt1 1 beta1 gt vt leftarrow beta2 vt1 1 beta2 gt2 bias correction hat mt leftarrow mt 1 beta1t hat vt leftarrow vt 1 beta2t update parameter thetat leftarrow thetat1 eta hat mt sqrthat vt epsilon memory using adam increase amount storage 2textnumparams thetatgt 4textnumparams thetatgtmtvt adafactor shazeer stern 2018 proposed way reduce memory footprint instead storing moment mtvt om time n matrix store row adafactor shazeer stern 2018 proposed way reduce memory footprint instead storing moment mtvt om time n matrix store row column sum om n memory reconstruct matrix remove momentum used train t5 difficult get adafactor train see twitter thread blog post mixedprecision training another method reducing memory narang et al 2018 default fp32 32bit floating point option fp16 16bit floating point problem value le 224 becomes 0 solution store master weight fp32 everything else fp16 loss scaling scale loss avoid gradient small magnitude result half memory usage learning rate normally learning rate decrease time transformer actually need increase learning rate warmup huang et al 2020 show potential reason prevent vanishing learning rate decrease time transformer actually need increase learning rate warmup huang et al 2020 show potential reason prevent vanishing gradient layer normalization lead instability adam optimizer initialization given matrix w rm time n standard initialization xavier initialization wij sim sn0 1n n fanin gpt2 gpt3 scale weight additional 1sqrtn n number residual layer t5 scale attention matrix additional 1sqrtd code gpt3 adam parameter beta1 09 beta2 095 epsilon 108 batch size 32 million token 1500 sequence use gradient clipping gt leftarrow gt min1 g2 linear learning rate warmup first 375 million token cosine learning rate go 10 value gradually increase batch size weight decay leftarrow gt min1 g2 linear learning rate warmup first 375 million token cosine learning rate go 10 value gradually increase batch size weight decay 01 reading mixed precision training fixing weight decay regularization adam loshchilov f hutter 2017 introduces adamw electra pretraining text encoders discriminator rather generator kevin clark minhthang luong quoc v le christopher manning iclr 2020 deberta decodingenhanced bert disentangled attention pengcheng xiaodong liu jianfeng gao weizhu chen iclr 2020