chapter 8 transformer transformer recent family architecture revolutionized field like natural language processing nlp image processing multimodal generative ai transformer originally introduced field nlp 2017 approach process understand human language human language inherently sequential nature eg character form word word form sentence sentence form paragraph document prior advent transformer architecture recurrent neural net work rnns briefly dominated field ability process sequential information rnns described appendix c reference however rnns like many ar chitectures processed sequential information iterativesequential fashion whereby item sequence individually processed one another transformer offer many advantage rnns including ability process item sequence item sequence individually processed one another transformer offer many advantage rnns including ability process item sequence parallel fashion cnns like cnns transformer factorize signal processing problem stage volve independent identically processed chunk however also include layer mix information across chunk called attention layer full pipeline model dependency chunk chapter describe transformer bottom start idea embeddings token section 81 describe attention mechanism sec tion 82 finally assemble idea together arrive full trans former architecture section 83 81 vector embeddings token understand attention mechanism detail need first introduce former architecture section 83 81 vector embeddings token understand attention mechanism detail need first introduce new data structure new way thinking neural processing language field nlp aim represent word vector aka word embeddings capture semantic meaning precisely degree two word related realworld u human reflected corresponding vector term numeric value word dog cat represented vector similar one another say cat table nowadays also typical every individual occurrence word distinct repre sentationvector story dog may mention word dog dozen time 70 mit 6390 spring 2024 71 vector slightly different based context sentence story large 70 mit 6390 spring 2024 71 vector slightly different based context sentence story large measure similar two word embeddings term numeric val ues common use cosine similarity metric utv u v co u v 81 u v length vector u v angle u v cosine similarity 1 u v zero two vector perpendicular 1 two vector diametrically opposed thus higher value correspond vector numerically similar word embeddings various approach create existed decade first approach produced astonishingly effective word embeddings word2vec 2012 revolutionary approach first highlysuccessful approach applying deep learning nlp enabled subsequent progress field includ word2vec 2012 revolutionary approach first highlysuccessful approach applying deep learning nlp enabled subsequent progress field includ ing transformer detail word2vec beyond scope course note two fact 1 created single word embedding distinct word training cor pu peroccurrence basis 2 produced word embeddings useful many relationship vector corresponded realworld semantic related ness example using euclidean distance distance metric two vector word2vec produced word embeddings property vword vector word vparis vfrance vitaly vrome 82 corresponds realworld property paris france rome italy incredible finding existed geographic word sort real 82 corresponds realworld property paris france rome italy incredible finding existed geographic word sort real world concept vocabulary nevertheless extent exact value embedding arbitrary matter holistic relation embed ding along performantuseful exact task care example embedding may considered good accurately capture con ditional probability given word appear next sequence word probably good idea word might typically fill blank end sentence rain grass model could built try correctly predict word middle sentence child fell long car ride model built minimizing loss function penalizes incorrect word guess child fell long car ride model built minimizing loss function penalizes incorrect word guess reward correct one done training model large corpus written material wikipedia even accessible digitized written material produced human dive full detail tokenization highlevel idea straight forward individual input data represented processed model referred token instead processing word whole word typically split smaller meaningful piece akin syllable thus refer token know referring individual input practice nowadays tend subwords eg word talked may split two token talk ed last updated 041124 124131 mit 6390 spring 2024 72 82 query key value attention tend subwords eg word talked may split two token talk ed last updated 041124 124131 mit 6390 spring 2024 72 82 query key value attention attention strategy processing global information efficiently focusing part signal salient task hand present low socalled dotproduct attention mechanism variant involve complex attention function might help understanding attention mechanism think dictio nary lookup scenario consider dictionary key k mapping value vk example let k name food pizza apple sandwich donut chili burrito sushi hamburger corresponding value may information food available much cost ingredient chili burrito sushi hamburger corresponding value may information food available much cost ingredient suppose instead looking food specific name wanted query cuisine eg mexican food clearly simply look word mexican among dictionary key since word food work utilize idea finding similarity vector embeddings query key end result wed hope get probability distribution food pkq indicating best match given query q distribution look key semantically close given query concretely get distribution follow step first embed word interested mexican example socalled query vector denoted simply q rdk1 dk embedding dimension interested mexican example socalled query vector denoted simply q rdk1 dk embedding dimension next suppose given dictionary n number entriesentries embed one socalled key vector particular jth entry dictio nary produce kj rdk1 key vector j 1 2 3 n obtain desired probability distribution using softmax see chapter 6 applied innerproduct key query pkq softmax qtk1 qtk2 qtk3 qtkn vectorbased lookup mechanism come known attention sense pkq conditional probability distribution say much attention given key kj given query q word conditional probability distribution pkq give attention weight weighted average value x j pkjq vj 83 attention output word conditional probability distribution pkq give attention weight weighted average value x j pkjq vj 83 attention output meaning weighted average value may ambiguous value word however attention output really becomes meaningful value projected semantic embedding space projection typically done transformer via learned embedding weight weightedsum idea generalizes multiple query key value particu lar suppose nq number query nk number key therefore nk number value one compute attention matrix softmax q 1 k1 q 1 k2 q 1 knk dk softmax q 2 k1 q 2 k2 q 2 knk dk softmax q nqk1 q nqk2 q nqknk dk 84 softmaxj softmax nkdimensional vector indexed j eq 82 q 2 k1 q 2 k2 q 2 knk dk softmax q nqk1 q nqk2 q nqknk dk 84 softmaxj softmax nkdimensional vector indexed j eq 82 mean softmax computed key equation normalization dk last updated 041124 124131 mit 6390 spring 2024 73 done reduce magnitude dot product would otherwise grow undesir ably large increasing dk making difficult overall training let ij entry ith row jth column attention matrix ij help answer question token xj help predicting corresponding output token yi attention output given weighted sum value yi n x j1 ijvj 821 self attention selfattention attention mechanism key value query gener ated input yi n x j1 ijvj 821 self attention selfattention attention mechanism key value query gener ated input high level typical transformer selfattention layer map rnd rnd particular transformer take data sequence token x rnd token xi rd1 computes via learned projection discussed section 831 query qi rdq1 key ki rdk1 value vi rdv1 practice dq dk dv often denote three embedding dimension via unified dk note dk differs dimen sion raw input token rdq1 selfattention layer take query key value compute self attention matrix softmax q 1 k1 q 1 k2 q 1 kn dk softmax q 2 k1 q 2 k2 q 2 kn dk softmax q nk1 q nk2 q nkn dk 85 attention matrix softmax q 1 k1 q 1 k2 q 1 kn dk softmax q 2 k1 q 2 k2 q 2 kn dk softmax q nk1 q nk2 q nkn dk 85 comparing selfattention matrix attention matrix described equation 82 notice difference lie dimension since selfattention query key value come input nq nk nv often denote three unified n selfattention output given weighted sum value yi n x j1 ijvj diagram show middle input token generating query combined key computed token generate attention weight via softmax output softmax combined value computed ken generate attention output corresponding middle input token repeating input token generates output ken generate attention output corresponding middle input token repeating input token generates output last updated 041124 124131 mit 6390 spring 2024 74 study question five colored token diagram gray blue gange green red could read diagram correspondence color input query query value output note size output size input also observe apparent notion ordering input word depicted structure posi tional information added encoding number token giving say token position relative start sequence vector embedding token note given query need pay attention token input example token used query used key value note given query need pay attention token input example token used query used key value generally mask may applied limit token used attention computation example one common mask limit attention computation token occur previously time one used query prevents atten tion mechanism looking ahead scenario transformer used generate one token time selfattention stage trained key value query embeddings lead pay specific attention particular feature input generally want pay attention many different kind feature input example translation one feature might verb another might object subject transformer utilizes multiple instance selfattention known attention head allow one feature might verb another might object subject transformer utilizes multiple instance selfattention known attention head allow combination attention paid many different feature 83 transformer transformer composition number transformer block multiple attention head highlevel goal transformer block output last updated 041124 124131 mit 6390 spring 2024 75 really rich useful representation input token sake high performant whatever task model trained learn rather depicting transformer graphically worth returning beauty underlying equations1 831 learned embedding simplicity assume transformer internally us selfattention full general atten tion layer work similarly formally transformer block parameterized function f map rnd rnd tion layer work similarly formally transformer block parameterized function f map rnd rnd input data x rnd often represented sequence n token token xi rd1 three projection matrix weight wq wk wv learned token xi rd1 produce 3 distinct vector query vector qi wt qxi key vector ki wt kxi value vector vi wt v xi 3 vector rdk1 learned weight wq wk wv rddk stack n query key value vector matrix form q rndk k rndk v rndk compactly write learned transformation sequence input token x q xwq k xwk v xwv q k v triple used produce one selfattentionlayer output one layer called one attention head one one attention head query key value layer called one attention head one one attention head query key value embedded via encoding matrix qh xwhq 86 kh xwhk 87 vh xwhv 88 whq whk whv rddkwhere dk size keyquery embedding space h 1 h index attention head attentionhead h learn one set whq whk whv perform weighted sum output head ui h x h1 wt hc n x j1 h ij vh j 89 whc rdkd ui rd1 index 1 n j 1 n integer index token vh j dk 1 value embedding vector corresponds input token xj attention head h standardized combined xi using layernorm function defined become ui layernorm xi ui 1 1 810 parameter 1 1 rd head h standardized combined xi using layernorm function defined become ui layernorm xi ui 1 1 810 parameter 1 1 rd 1the presentation follows note john thickstun last updated 041124 124131 mit 6390 spring 2024 76 get final output follow intermediate output layer norm recipe particular first get transformer block output zi given zi wt 2 relu wt 1 ui 811 weight w1 rdm w2 rmd standardized combined ui give final output zi zi layernorm ui zi 2 2 812 parameter 2 2 rd vector assembled eg parallel computation produce z rnd layernorm function transforms ddimensional input z parameter rd layernormz z z z 813 z mean z standard deviation z z 1 x i1 zi layernormz z z z 813 z mean z standard deviation z z 1 x i1 zi 814 z v u u 1 x i1 zi z2 815 layer normalization done improve convergence stability training model parameter comprise weight matrix whq whk whv whc w1 w2 layernorm parameter 1 2 1 2 transformer composition l trans former block parameter fl f2 f1x rnd 816 hyperparameters model dk h l 832 variation training many variant transformer structure exist example layernorm may moved stage neural network sophisticated attention function may employed instead simple dot product used eq 82 transformer may also used pair example one process input separate one gen may employed instead simple dot product used eq 82 transformer may also used pair example one process input separate one gen erate output given transformed input selfattention may also replaced crossattention input data used generate query input data generate key value positional encoding masking also common though left implicit equation simplicity transformer trained number parameter large mod ern transformer model like gpt4 ten billion parameter great deal data thus necessary train model else model may simply overfit small datasets training large transformer model thus generally done two stage first pre training stage employ large dataset train model extract pattern datasets training large transformer model thus generally done two stage first pre training stage employ large dataset train model extract pattern done unsupervised selfsupervised learning unlabelled data example wellknown bert model pretrained using sentence word masked last updated 041124 124131 mit 6390 spring 2024 77 model trained predict masked word bert also trained sequence sentence model trained predict whether two sentence likely contextually close together pretraining stage generally expensive second finetuning stage train model specific task classification question answering training stage relatively inexpensive generally requires labeled data last updated 041124 124131 transformer large language model introduction large language model requires labeled data last updated 041124 124131 transformer large language model introduction large language model language model remember simple ngram language model assigns probability sequence word generate text sampling possible next word trained count computed lot text large language model similar different assigns probability sequence word generate text sampling possible next word trained learning guess next word neural large language model llm selfsupervised learner take text remove word use neural model guess word model wrong use stochastic gradient descent make model guess better next time advantage need lot text gpt3 500 billion token lot compute llm built transformer transformer specific kind network architecture like advantage need lot text gpt3 500 billion token lot compute llm built transformer transformer specific kind network architecture like fancier feedforward network based attention provided proper attribution provided google hereby grant permission reproduce table gures paper solely use journalistic scholarly work attention need ashish vaswani google brain avaswanigooglecom noam shazeer google brain noamgooglecom niki parmar google research nikipgooglecom jakob uszkoreit google research uszgooglecom llion jones google research lliongooglecom aidan n gomez university toronto aidancstorontoedu ukasz kaiser google brain lukaszkaisergooglecom illia polosukhin illiapolosukhingmailcom cscl 2 aug 2023 approximate timeline 1990 static word embeddings 2003 neural language model 2008 multitask learning 2015 attention 2017 transformer illiapolosukhingmailcom cscl 2 aug 2023 approximate timeline 1990 static word embeddings 2003 neural language model 2008 multitask learning 2015 attention 2017 transformer 2018 contextual word embeddings pretraining 2019 prompting picture transformer language model input embeddings layer transformer block softmax vocabulary long thanks long thanks output linear layer transformer large language model introduction large language model transformer large language model attention instead starting big picture input embeddings layer transformer block softmax vocabulary long thanks long thanks output linear layer input embeddings layer transformer block softmax vocabulary long thanks long thanks output linear layer let consider embeddings individual word particular layer transformer block softmax vocabulary long thanks long thanks output linear layer let consider embeddings individual word particular layer problem static embeddings word2vec static embedding word doesnt reflect meaning change context chicken didnt cross street tired meaning represented static embedding contextual embeddings intuition representation meaning word different different context contextual embedding word different vector express different meaning depending surrounding word compute contextual embeddings attention contextual embeddings chicken didnt cross street property chicken didnt cross street tired chicken didnt cross street trafficy property chicken didnt cross street tired chicken didnt cross street trafficy point sentence probably referring either chicken street intuition attention build contextual embedding word selectively integrating information neighboring word say word attends neighboring word others intuition attention test animal didnt cross street tired animal didnt cross street tired layer 6 layer 5 selfattention distribution attention definition mechanism helping compute embedding token selectively attending integrating information surrounding token previous layer formally method weighted sum vector attention lefttoright selfattention layer x1 a1 x2 a2 a3 a4 a5 x3 x4 x5 layer formally method weighted sum vector attention lefttoright selfattention layer x1 a1 x2 a2 a3 a4 a5 x3 x4 x5 simplified version attention sum prior word weighted similarity current word given sequence token embeddings x1 x2 x3 x4 x5 xi produce ai weighted sum x1 x5 weighted similarity xi word word since representation make use old friend dot product used larity chapter 6 also played role attention result comparison word j equation add attention computation erson 1 scorexixj xi xj 104 oduct scalar value ranging larger vector compared continuing computing y3 would compute three score x3 x1 verson 1 scorexixj xi xj 1 esult dot product scalar value ranging la computing y3 would compute three score x3 x1 verson 1 scorexixj xi xj 1 esult dot product scalar value ranging la similar vector compared continuing rst step computing y3 would compute three score x3 x3 x3 make effective use score well normalize oftmax create vector weight aij indicates proporti input input element current focus attentio aij softmaxscorexixj 8j 1 expscorexixj pi k1 expscorexixk 8j 1 er large language model weighted value ai x ji aijxj 107 intuition attention test animal didnt cross street tired animal didnt cross street tired layer 6 layer 5 selfattention distribution x1 x2 x3 x4 x5 x6 x7 xi intuition attention test animal didnt cross street cross street tired layer 6 layer 5 selfattention distribution x1 x2 x3 x4 x5 x6 x7 xi intuition attention test animal didnt cross street tired animal didnt cross street tired layer 6 layer 5 selfattention distribution x1 x2 x3 x4 x5 x6 x7 xi query value attention actually slightly complicated wont get highlevel idea instead query set value embedding actually also key intuition attention test animal didnt cross street tired animal didnt cross street tired layer 6 layer 5 selfattention distribution x1 x2 x3 x4 x5 x6 x7 xi query value k v k v k v k v k v k v k v key summary attention method enriching representation token incorporating contextual information x1 x2 x3 x4 x5 x6 x7 xi query value k v k v k v k v k v k v k v key summary attention method enriching representation token incorporating contextual information result embedding word different different context contextual embeddings representation word meaning context transformer large language model attention transformer large language model rest transformer applied language modeling transformer attention part computing embeddings transformer let see mechanism reminder transformer language model input embeddings layer transformer block softmax vocabulary long thanks long thanks output linear layer residual stream token get passed modified layer norm xi hi1 layer norm multihead attention feedforward xi1 xi1 hi hi1 well need nonlinearities feedforward layer residual stream token get passed modified layer norm xi hi1 layer norm multihead attention feedforward xi1 xi1 hi hi1 well need nonlinearities feedforward layer layer norm xi hi1 layer norm multihead attention feedforward xi1 xi1 hi hi1 transformer stack block layer norm xi hi1 layer norm multihead attention feedforward xi1 xi1 hi hi1 layer norm xi hi1 layer norm multihead attention feedforward xi1 xi1 hi hi1 block 1 block 2 input x composite embeddings word position transformer block janet 1 2 back 3 janet back bill 4 bill 5 position embeddings word embeddings language modeling head layer l transformer block softmax vocabulary v unembedding layer 1 x v logits word probability 1 x v hl 1 w1 w2 wn hl 2 hl n x v 1 x unembedding layer et y1 y2 yv u1 u2 uv language model head take hl n output 1 x v logits word probability 1 x v hl 1 w1 w2 wn hl 2 hl n x v 1 x unembedding layer et y1 y2 yv u1 u2 uv language model head take hl n output distribution vocabulary v final transformer model x x1 x2 xn multihead selfattention layer norm feedforward layer norm layer 1 h1 h2 hn multihead selfattention layer norm feedforward layer norm layer 2 h1 h2 hn multihead selfattention layer norm feedforward layer norm layer l h1 h2 hn sample token generate position n1 wn1 w1 w2 wn input token p1 p2 pn add token position embeddings language model head token probability y1 y2 yv ew1 ew2 ewn transformer large language model rest transformer applied language modeling large language model pretraining train transformer language modeling pretraining big idea underlies amazing applied language modeling large language model pretraining train transformer language modeling pretraining big idea underlies amazing performance language model first pretrain transformer model enormous amount text apply new task intuition language model training train predict next word 1 take corpus text 2 time step ask model predict next word ii train model using gradient descent minimize error prediction intuition language model training loss loss function crossentropy loss want model assign high probability true word w want loss high model assigns low probability w ce loss negative log probability model assigns true next word w model assigns low probability w move model weight direction assigns ce loss negative log probability model assigns true next word w model assigns low probability w move model weight direction assigns higher probability w training transformer language model input embeddings transformer block softmax vocabulary long thanks long thanks next word loss linear layer pretraining data mostly web common crawl pile figure 1 treemap pile component effective size model learn pretraining canine everywhere one dog front room two dog wasnt big enormous author room one virginia woolf doctor told square root 4 2 big idea text contains enormous amount knowledge pretraining lot text knowledge give language model ability much large language model big idea text contains enormous amount knowledge pretraining lot text knowledge give language model ability much large language model pretraining train transformer language modeling large language model large language model applying pretrained model new task big idea many task turned task predicting word three architecture large language model decoder encoders encoderdecoders gpt claude bert family flant5 whisper llama 2 hubert mixtral pretraining three type architec neural architecture influence type pretra encoders get bidirectio tra encoder decoder good part whats be 32 decoder encoders encoder decoder 32 decoder language model weve see nice generate cant condit encoder decoder good part decoder encode whats best way pretrain decoder decoder 32 decoder language model weve see nice generate cant condit encoder decoder good part decoder encode whats best way pretrain decoder also called causal llm autoregressive llm lefttoright llm predict word left right 32 decoder conditional generation generating text conditioned previous text prex text completion text input embeddings transformer block sample softmax long thanks linear layer framing lot task conditional generation sentiment analysis like jackie chan 1 give language model string sentiment sentence like jackie chan 2 see word think come next prex text ut ding long thanks autoregressive text completion transformerbased large language model word negative see higher ppositivethe sentiment sentence like jackie chan autoregressive text completion transformerbased large language model word negative see higher ppositivethe sentiment sentence like jackie chan pnegativethe sentiment sentence like jackie chan word positive probable say sentiment senten positive otherwise say sentiment negative framing lot task conditional generation qa wrote origin specie 1 give language model string 2 see word think come next word positive probable say sentiment sente positive otherwise say sentiment negative also cast complex task word prediction consider th answering simple question task return chapter 14 ta system given question must give textual answer cast th question answering word prediction giving language model questio system given question must give textual answer cast th question answering word prediction giving language model questio token like suggesting answer come next q wrote book origin specie ask language model compute pwq wrote book origin specie look word w high probability might expect se charles likely choose charles continue ask pwq wrote book origin specie charles word positive probable say sentiment sent sitive otherwise say sentiment negative also cast complex task word prediction consider answering simple question task return chapter 14 stem given question must give textual answer cast question answering word prediction giving language model quest stem given question must give textual answer cast question answering word prediction giving language model quest oken like suggesting answer come next q wrote book origin specie ask language model compute pwq wrote book origin specie look word w high probability might expect h l er likel e choose ch l contin e ask encoders many variety popular masked language model mlms bert family trained predicting word surrounding word side usually finetuned trained supervised data classification task neural architecture influence decoder encoders encoder decoder encoderdecoders trained map one sequence another popular machine translation map one language another speech recognition map acoustic word 32 e trained map one sequence another popular machine translation map one language another speech recognition map acoustic word 32 e many thing didnt talk instruction finetuning preference alignment prompt engineering learn cs224n large language model large language model applying pretrained model new task large language model harm large language model hallucination current research direction address hallucination retrievalaugmented generation rag use information retrieval retrieve passage highquality source use language model generate answer passage copyright privacy toxicity abuse misinformation vast growth interest ethic llm ai hai ai index 2023 large language model harm large language model large language model last class together learning goal write regular expression text task hai ai index 2023 large language model harm large language model large language model last class together learning goal write regular expression text task apply edit distance algorithm build supervised classifier build search engine work neural word embeddings train neural network build recommendation engine build chatbot prompt large language model whats next spring 2024 nlp adjacent course cs224n natural language processing deep learning chris manning algorithmic internals transformer gpt parsing machine translation application gory detail math machine learning cs224c nlp computational social science diyi yang machine learning theory social science study human behavior important societal question scale nlp social network causal inference application social topic like hate speech misinformation social movement important societal question scale nlp social network causal inference application social topic like hate speech misinformation social movement c 224s spoken language processing andrew maas introduction spoken language technology emphasis dialogue conversational system c 336 language modeling scratch tatsu hashimoto percy liang every aspect language model creation including data collection cleansing pre training transformer model construction model training evaluation deployment application required c 246 mining massive data set jure leskovec 65 next year course c 224v conversational virtual assistant deep learning monica lam topic include 1 growing llm knowledge 2 stopping llm hallucination 3 experimentation evaluation conversational assistant based llm 5 controlling llm achieve task 6 persuasive llm 7 multilingual assistant 3 experimentation evaluation conversational assistant based llm 5 controlling llm achieve task 6 persuasive llm 7 multilingual assistant 8 combining voice graphical interface cs329x human centered nlp diyi yang humancentered design thinking nlp humanintheloop algorithm fairness accessibility cs329r race nlp dan jurafsky jennifer eberhardt integrate method natural language processing social psychological perspective race build practical system address significant societal issue 66 fun course outside c spring linguist 173 invented language linguist 134a structure discourse linguist 156 language gender sexuality comm 154 politics algorithm next year linguistics 150 language society linguistics 130a introduction semantics pragmatic 67 l18 pretraining large language model llm co 484 natural language processing spring 2023 linguistics 150 language society linguistics 130a introduction semantics pragmatic 67 l18 pretraining large language model llm co 484 natural language processing spring 2023 recap pretraining finetuning pretrain model large dataset task x finetune dataset task finetuning process taking network learned pretrained model training model often via added neural net classifier take top layer network input perform downstream task finetuning training process take gradient descent step recap pretraining finetuning 3652 image 11 class 128m image 1000 class pretraining finetuning pretraining finetuning natural language mask nlp mask subfield linguistics computer science artificial mask concerned interaction mask computer human mask processing interdisciplinary subfield linguistics computer science artificial mask concerned interaction mask computer human mask processing interdisciplinary intelligence language 33b token 512 token per segment contains wit labored gag greatest musician good viewing alternative negative positive positive 67k example 2 class recap pretraining finetuning experiment glue wang et al 2019 example range 25k 392k example today going see us pretrained model 1 fewshot example eg 32 2 finetuning gradient update lecture postbert model pretraining finetuning gpt3 prompting incontext learning instruction tuning rlhf chatgpt gpt4 limitation llm postbert model pretrainingfinetuning roberta bert still undertrained removed next sentence prediction pretraining add noise benefit limitation llm postbert model pretrainingfinetuning roberta bert still undertrained removed next sentence prediction pretraining add noise benefit trained longer 10x data bigger batch size pretrained 1024 v100 gpus one day 2019 7 liu et al 2019 roberta robustly optimized bert pretraining approach albert 8 lan et al 2020 albert lite bert selfsupervised learning language representation albert model le parameter le storage slower model architecture larger key idea parameter sharing across different layer smaller embedding size distillbert tinybert mobilebert 9 httpsgithubcomabhilash1910 distilbertsquadv1notebook key idea produce smaller model student distill information bert model teacher sanh et al 2019 distilbert distilled version bert smaller faster cheaper lighter electra 10 information bert model teacher sanh et al 2019 distilbert distilled version bert smaller faster cheaper lighter electra 10 clark et al 2020 electra pretraining text encoders discriminator rather generator electra provides efficient training method predicts 100 token instead 15 every time discriminator used downstream finetuning three major form pretraining 11 texttotext model masked language model auto regressive language model httpswwwfactoredai20210921anintuitiveexplanationof transformerbasedmodels masked language model transformer encoder autoregressive language model transformer decoder texttotext model transformer encoderdecoder texttotext model best world bar encoderonly model eg bert enjoy benefit bidirectionality cant used generate text transformer encoderdecoder texttotext model best world bar encoderonly model eg bert enjoy benefit bidirectionality cant used generate text decoderonly model eg gpt generation lefttoright lm texttotext model combine best world 12 raffel et al 2020 exploring limit transfer learning unified texttotext transformer t5 texttotext transfer transformer t5 model 13 raffel et al 2020 exploring limit transfer learning unified texttotext transformer encoder decoder use pretrained model 14 gpt3 prompting incontext learning gpt gpt 2 gpt 3 decoderonly transformerbased language model model size training corpus radford et al 2019 language model unsupervised multitask learner gpt2 trained 40gb internet text context size 1024 gpt model size training corpus radford et al 2019 language model unsupervised multitask learner gpt2 trained 40gb internet text context size 1024 gpt 2 started achieve strong zeroshot performance radford et al 2019 language model unsupervised multitask learner httpstransformerhuggingfacecodocgpt2large gpt 3 language model fewshot learner brown et al 2020 language model fewshot learner context size 2048 gpt2 gpt3 15b 175b parameter 14b 300b token paradigm shift since gpt 3 gpt3 finetuning default way learning model like bertt5gpt2 sst2 67k example squad 88k passage answer question triple finetuning requires computing gradient applying parameter update every example every k example minibatch however expensive 175b gpt3 model gpt 3 fewshot learning applying parameter update every example every k example minibatch however expensive 175b gpt3 model gpt 3 fewshot learning gpt3 proposes alternative incontext learning forward pas gradient update need feed small number example eg 32 hand cant feed many example bounded context size gpt 3 task specification drop reading comprehension task unscrambling word word context wic 22 brown et al 2020 language model fewshot learner gpt 3 incontext learning gpt 3 performance superglue 23 wang et al 2019 superglue stickier benchmark generalpurpose language understanding system 24 gpt 3 incontext learning brown et al 2020 language model fewshot learner httpaistanfordedublogincontextlearning chainofthought cot prompting 25 24 gpt 3 incontext learning brown et al 2020 language model fewshot learner httpaistanfordedublogincontextlearning chainofthought cot prompting 25 wei et al 2022 chainofthought prompting elicits reasoning large language model emergent property llm 26 wei et al 2022 emergent ability large language model happened gpt3 model size training corpus way go chatgpt developed httpsyaofunotionsitehowdoesgptobtainitsabilitytracingemergent abilitiesoflanguagemodelstotheirsources b9a57ac0fcf74f30a1ab9e3e36fa1dc1 whats new training code supervised instruction tuning rlhf reinforcement learning human feedback chatgpt developed slide credit graham neubig instructgpt supervised instruction tuning rlhf ouyang et al 2022 training language model follow instruction human feedback 31 sft data 13k public instructgpt supervised instruction tuning rlhf ouyang et al 2022 training language model follow instruction human feedback 31 sft data 13k public supervised instruction tuning ouyang et al 2022 training language model follow instruction human feedback instructgpt supervised instruction tuning rlhf instructgpt supervised instruction tuning rlhf ouyang et al 2022 training language model follow instruction human feedback chatgpt instructgpt dialogue data httpsopenaicomblogchatgpt trained model using reinforcement learning human feedback rlhf using method instructgpt slight difference data collection setup trained initial model using supervised fine tuning human ai trainer provided conversation played sidesthe user ai assistant gave trainer access modelwritten suggestion help compose tuning human ai trainer provided conversation played sidesthe user ai assistant gave trainer access modelwritten suggestion help compose response mixed new dialogue dataset instructgpt dataset transformed dialogue format human feedback data key recent model getting smaller 35 smaller model trained 14t highquality publicly available data model public llama13b outperforms gpt3 175b benchmark llama65b competitive best model chinchilla70b palm540b touvron et al 2023 llama open efficient foundation language model gpt 4 gpt4 multimodal model capable processing image text input producing text output model size training detail unknown process 32k context size 36 37 gpt 4 limitation llm 39 llm knowledge retriever llm costly update 40 text output model size training detail unknown process 32k context size 36 37 gpt 4 limitation llm 39 llm knowledge retriever llm costly update 40 update llm uptodate world knowledge llm easy hallucinate generate factuallyincorrect text 41 llm easy hallucinate 42 bang et al 2023 multitask multilingual multimodal evaluation chatgpt reasoning hallucination interactivity llm easy hallucinate llm easy hallucinate generate factuallyincorrect text solution generate text citation factchecked 43 llm easy hallucinate httpsopenaicomresearchwebgpt 44 prune 175b model 50 unstructured pruning without loss perplexity quantization seems promising memory reduction 2x llm costly train deploy project design planning ankur taly google john mitchell stanford anupam datta trueracmu quantization seems promising memory reduction 2x llm costly train deploy project design planning ankur taly google john mitchell stanford anupam datta trueracmu cs329t trustworthy machine learning large language model application lecture 6 fall 2023 last three lecture llm education personalized tutor grading teacher assistance llm security llm security defense fuzzing bug finding code analysis decompilation attack spear phishing craft code exploit vulnerability security llm defending measure used trust break alignment adversarial prompt llm healthcare extract structure ehr medical coding search qa key step llm x project frame task prompt tune model evaluate model ass reliability trustworthiness expanded next two slide evaluate model ass reliability trustworthiness expanded next two slide key step llm x project 1 frame task application functionality needed input output prompt tune model design prompt include fewshot example finetune weight training data retrieval augmented generation rag supplying relevant context evaluate model gather evaluation datasets define set metric tricky generative task may canonical label think would make good response key step llm x project 2 ass reliability trustworthiness grounding ensure response always grounded knowledge source made confidence quantify uncertainty confidence response grounding ensure response always grounded knowledge source made confidence quantify uncertainty confidence response interpretability understand model generated response robustness ass robustness model adversarial prompt example better homework grading 1 frame task application tool grading homework exam question given rubric functionality needed accurately score solution give useful comment input output input question solution evaluate grading instruction rubric output score explanation comment prompt tune model let assume existing work accurate scoring try generate useful comment build llm explanation program error see llm explain homework error example better homework grading 2 evaluate model build llm explanation program error see llm explain homework error example better homework grading 2 evaluate model gather evaluation datasets may existing grade data stanford c class define set metric tricky generative task controversial work mit using llm grading exam possibly use llm evaluate explanation example better homework grading 3 ass reliability trustworthiness interpretability score explanation explanation score faithful solution instruction eg explanation say solution mention x actually x irrelevant problem hand confidence certain model score robustness could student cheat slightly tweaking incorrect solution accepted llm correct example summarize doctor note 1 frame task robustness could student cheat slightly tweaking incorrect solution accepted llm correct example summarize doctor note 1 frame task application tool summarizing doctor note functionality needed comprehensively faithfully summarize doctor note input output input free text doctor note output table extracting various dimension prescription diagnosis lab ordered note prompt tune model write prompt describing task include example prompt model understand expected form output example summarize doctor note 2 evaluate model gather evaluation datasets may existing structured summarization datasets domain eg retail product description feature spec define set metric precision recall identified feature domain eg retail product description feature spec define set metric precision recall identified feature example summarize doctor note 3 ass reliability trustworthiness grounding every identified feature summary must present note recall diabetes example divyas talk interpretability feature part note featurevalue pair coming robustness robust model seemingly benign perturbation note eg add drop punctuation add drop stop word robust model order information note instance prescription mentioned diagnosis reliability trustworthiness grounding confidence interpretability robustness expanded next four slide grounding grounding confidence interpretability robustness expanded next four slide grounding llm response considered grounded every claim response attributed authoritative knowledge source two part improving groundedness response retrieval augmented generation prompt model use information beyond context verifying groundness response use natural language inference nli model compare response hypothesis context premise read true reevaluating factual consistency evaluation sample multiple response check consistency read selfcheckgpt zeroresource blackbox hallucination detection generative large language model condence establish level condence certainty llm response quantify using calibrated numerical probability score read large language model condence establish level condence certainty llm response quantify using calibrated numerical probability score read semantic uncertainty linguistic invariance uncertainty estimation natural language generation ask calibration strategy eliciting calibrated condence score language model finetuned human feedback teaching model express uncertainty word incorporate uncertainty response text sure answer may read reducing conversational agent overcondence linguistic calibration interpretability understandexplaininterpret model came response tracing response part prompt perturbation ablation shapley value read explanation game explaining machine learning model using shapley value anchor highprecision modelagnostic explanation examine gradient input token read explanation game explaining machine learning model using shapley value anchor highprecision modelagnostic explanation examine gradient input token read axiomatic attribution deep network tracing response training netuning set inuence function tracin requires access training checkpoint read understanding blackbox prediction via inuence function studying large language model generalization inuence function robustness examine model robust adversarial input design adversarial input fool model making model return incorrect outcome read semantically equivalent adversarial rule debugging nlp model beyond accuracy behavioral testing nlp model checklist make model generate bad racist abusive inapproprirate text read exploiting programmatic behavior llm dualuse standard security attack make model generate bad racist abusive inapproprirate text read exploiting programmatic behavior llm dualuse standard security attack design mitigation guard adversarial input input lters output lters model tuning often using human feedback rlhf sample education project direction clara writing assistant google doc bruno conversation transcript tool llm analysis coding error message explanation contrasting case designed teacher tool would work student detail later slide enhanced error message gpt compare two approach baseline option generate explanatory error message using openais gpt real time construct error message link course discussion forum result student using gptgenerated error message generate explanatory error message using openais gpt real time construct error message link course discussion forum result student using gptgenerated error message repeat error 235 le often subsequent attempt resolve error 361 fewer additional attempt compared standard error message wang mitchell piech aigenerated teaching example mah levine contrasting case general concept best illustrated using two contrasting example additional education project idea lesson planning teachassist riz malik neurodiversity adhd flexable ai httpsedstanfordeduldtstudentsprojectsflexableai neurodiversity creativity bangladesh labib rahman language learning kate ted song httpsedstanfordeduldtstudentsprojectskateknowledgeableaitutoringenglish bangladesh labib rahman language learning kate ted song httpsedstanfordeduldtstudentsprojectskateknowledgeableaitutoringenglish learner teacher study buddy olivia tomaneo httpsedstanfordeduldtstudentsprojectsstudybuddy personalized early reading ello c grad working nick haber httpswwwhelloellocomlpsreadingconfidenceutmsourcebingutmmediumcpcutmcampaignbsbrandut mtermellomsclkid9316c8ce6f2319a326efadd7c66c8de6 project read ramakrishnan gsb httpswwwgsbstanfordeduexperiencenewshistoryvivekramakrishnanmba23howaicouldhelpsolveschoollite racycrisis intersection vrai engagement alex stolyarik launching interesting program area unity led kristen blair ai literacy craft victor lee parth sarin httpscraftstanfordedu novice approach programming benjamin xie httpswwwbenjixiecompublicationicer2023icer2023pdf ai literacy craft victor lee parth sarin httpscraftstanfordedu novice approach programming benjamin xie httpswwwbenjixiecompublicationicer2023icer2023pdf sample security project direction llm fuzzing bug nding llm attacking model software people spear phishing craft code exploit vulnerability ass llm robustness attack technique breaking alignment defense technique detecting adversarial prompt detecting whether text llm generated technique watermarking examine structure llm probability function read watermark large language model detectgpt zeroshot machinegenerated text detection using probability curvature attack technique defeating detector read aigenerated text reliably detected detail ahead mitigating security risk workshop genai technology changed computing landscape read aigenerated text reliably detected detail ahead mitigating security risk workshop genai technology changed computing landscape enabled exciting application help adversary generate spearfishing email spread misinformation workshop risk genai considers could attacker leverage genai technology security measure change response genai technology current emerging technology designing countermeasure june workshop organized google stanford uwmadison second oct 16 coming stay tuned new idea security llm x open direction future research expert trying fix adversarial example computer vision ten year room experimentation repeat attack based open release experimental method compare possible defense look new idea example computer vision ten year room experimentation repeat attack based open release experimental method compare possible defense look new idea sample healthcare project direction extract structured data narrative text eg billing code note symptom narrative plan doctor nurse note generate narrative text structured data generate report nonnarratives imaging signal question answering detail later slide healthcare idea layerheath extract structured data narrative text eg billing code note symptom narrative plan doctor nurse note generate narrative text structured data generate report nonnarratives imaging signal combine text model data create model cohort selection generate narrative text structured data generate report nonnarratives imaging signal combine text model data create model cohort selection outcome prediction summarize vast number note whats important use case question answering dataset suggestion layerhealth dataset description clinical trial matching fda clinical trial eligibility criterion freely available online medical information mart intensive care mimic vast dataset deidentified structured unstructured clinical data across icu ed pmc patient patient summary extracted pubmed case report 167k patient adverse drug event corpus extract adverse drug event ade set clinical note synthetic note generation generate synthetic note student interest aman kansal msc student ai interested application llm healthcare edtech security pooja sethi generate synthetic note student interest aman kansal msc student ai interested application llm healthcare edtech security pooja sethi m c student hcp program interested intersection vision llm well multilingual model based remotely seattle happy hop video call kevin marx 2nd year ee master student focusing hardware bioinstrumentation interested medical diagnostics resource constrained environment jerry m learning design technology gse interested application llm edtech particularly related computer science education aryan siddiqui sophomore deeply involved llm space worked kick developing agent integrate financial platform interested security education thought forward jerry matt use llm build aiassisted code editor guide computer science student solve coding thought forward jerry matt use llm build aiassisted code editor guide computer science student solve coding question without spoiling solution contract constructionlaw typically long diicult locate exact piece information project focus building questionanswering system using llm plus retrieval based supplied document thought forward dora andrew promote transparency ml dataset collection one popular proposed method creating datasheets gebru et al propose leveraging llm grounded existing research document dataset generate datasheets disproportionate amount healthcare burden placed certain patient group eg black woman want empower patient advocacy using llm help practice conversation clinician research paper computer science machine learning often describe new algorithm conversation clinician research paper computer science machine learning often describe new algorithm may want implement use llm ingest content research paper produce code language choice discussion