introduction cs324 link search menu expand document cs324 home calendar lecture introduction capability harm harm ii data security legality modeling training parallelism scaling law selective architecture adaptation environmental impact paper review paper discussion project site us doc documentation theme jekyll lecture introduction newcommandsvmathcalv newcommandnl1textsf1 newcommandgenerate1stackrel1rightsquigarrow welcome cs324 new course understanding developing large language model language model brief history course exist structure course language model classic definition language model lm probability distribution sequence token suppose vocabulary sv set token language model p assigns sequence token x1 dot xl sv probability distribution sequence token suppose vocabulary sv set token language model p assigns sequence token x1 dot xl sv probability number 0 1 px1 dot xl probability intuitively tell u good sequence token example vocabulary sv nlate nlball nlcheese nlmouse nlthe language model might assign demo pnlthe nlmouse nlate nlthe nlcheese 002 pnlthe nlcheese nlate nlthe nlmouse 001 pnlmouse nlthe nlthe nlcheese nlate 00001 mathematically language model simple beautiful object simplicity deceiving ability assign meaningful probability sequence requires extraordinary implicit linguistic ability world knowledge example lm assign nlmouse ability assign meaningful probability sequence requires extraordinary implicit linguistic ability world knowledge example lm assign nlmouse cheese ate low probability implicitly ungrammatical syntactic knowledge lm assign nlthe mouse ate cheese higher probability nlthe cheese ate mouse implicitly world knowledge sentence syntactically differ semantic plausibility generation defined language model p take sequence return probability ass goodness also generate sequence given language model purest way sample sequence x1l language model p probability equal px1l denoted x1l sim p computationally efficiently depends form language model p practice generally probability equal px1l denoted x1l sim p computationally efficiently depends form language model p practice generally sample directly language model limitation real language model sometimes wish obtain average sequence something closer best sequence autoregressive language model common way write joint distribution px1l sequence x1l using chain rule probability px1l px1 px2 mid x1 px3 mid x1 x2 cdots pxl mid x1l1 prodi1l pxi mid x1i1 example demo beginalign pnlthe nlmouse nlate nlthe nlcheese pnlthe pnlmouse mid nlthe pnlate mid nlthe nlmouse pnlthe mid nlthe nlmouse nlate pnlcheese mid nlthe nlmouse nlate nlthe pnlmouse mid nlthe pnlate mid nlthe nlmouse pnlthe mid nlthe nlmouse nlate pnlcheese mid nlthe nlmouse nlate nlthe endalign particular pxi mid x1i1 conditional probability distribution next token xi given previous token x1i1 course joint probability distribution written way mathematically autoregressive language model one conditional distribution pxi mid x1i1 computed efficiently eg using feedforward neural network generation generate entire sequence x1l autoregressive language model p sample one token time given token generated far textfor 1 dot l hspace1in xi sim pxi mid x1i11t ge 0 temperature parameter control much randomness want language model 1 dot l hspace1in xi sim pxi mid x1i11t ge 0 temperature parameter control much randomness want language model 0 deterministically choose probable token xi position 1 sample normally pure language model infty sample uniform distribution entire vocabulary sv however raise probability power 1t probability distribution may sum 1 fix renormalizing distribution call normalized version ptxi mid x1i1 propto pxi mid x1i11t annealed conditional probability distribution example pnlcheese 04 quadquadquad pnlmouse 06 pt05nlcheese 031 quadquadquad pt05nlmouse 069 pt02nlcheese 012 quadquadquad pt02nlmouse 088 pt0nlcheese 0 06 pt05nlcheese 031 quadquadquad pt05nlmouse 069 pt02nlcheese 012 quadquadquad pt02nlmouse 088 pt0nlcheese 0 quadquadquad pt0nlmouse 1 aside annealing reference metallurgy hot material cooled gradually show sampling optimization algorithm simulated annealing technical note sampling iteratively temperature parameter applied conditional distribution pxi mid x1i11t equivalent except 1 sampling annealed distribution length l sequence conditional generation generally perform conditional generation specifying prefix sequence x1i called prompt sampling rest xi1l called completion example generating t0 produce demo underbracenlthe nlmouse nlatetextprompt called prompt sampling rest xi1l called completion example generating t0 produce demo underbracenlthe nlmouse nlatetextprompt generatet0 underbracenlthe nlcheesetextcompletion change temperature 1 get variety demo example nlits house nlmy homework well see shortly conditional generation unlocks ability language model solve variety task simply changing prompt summary language model probability distribution p sequence x1l intuitively good language model linguistic capability world knowledge autoregressive language model allows efficient generation completion xi1l given prompt x1i temperature used control amount variability generation brief history information theory entropy english ngram completion xi1l given prompt x1i temperature used control amount variability generation brief history information theory entropy english ngram model information theory language model date back claude shannon founded information theory 1948 seminal paper mathematical theory communication paper introduced entropy distribution hp sumx px log frac1px entropy measure expected number bit algorithm need encode compress sample x sim p bitstring nlthe mouse ate cheese rightarrow 0001110101 lower entropy structured sequence shorter code length intuitively log frac1px length code used represent element x occurs probability px px frac18 allocate log28 3 bit equivalently log8 length code used represent element x occurs probability px px frac18 allocate log28 3 bit equivalently log8 208 nats aside actually achieving shannon limit nontrivial eg ldpc code topic coding theory entropy english shannon particularly interested measuring entropy english represented sequence letter mean imagine true distribution p existence questionable still useful mathematical abstraction spout sample english text x sim p shannon also defined cross entropy hp q sumx px log frac1qx measure expected number bit nats needed encode sample x sim p using compression scheme given model q representing x code length frac1qx estimating entropy via language bit nats needed encode sample x sim p using compression scheme given model q representing x code length frac1qx estimating entropy via language modeling crucial property cross entropy hp q upper bound entropy hp hp q ge hp mean estimate hp q constructing language model q sample true data distribution p whereas hp generally inaccessible p english get better estimate entropy hp constructing better model q measured hp q shannon game human language model shannon first used ngram model q 1948 1951 paper prediction entropy printed english introduced clever scheme known shannon game q provided human nlthe mouse ate ho human arent good providing calibrated probability english introduced clever scheme known shannon game q provided human nlthe mouse ate ho human arent good providing calibrated probability arbitrary text shannon game human language model would repeatedly try guess next letter one would record number guess ngram model downstream application language model became first used practical application required generation text speech recognition 1970s input acoustic signal output text machine translation 1990s input text source language output text target language noisy channel model dominant paradigm solving task noisy channel model taking speech recognition example posit text sampled distribution p text becomes realized speech acoustic signal given speech wish recover likely example posit text sampled distribution p text becomes realized speech acoustic signal given speech wish recover likely text done via bayes rule ptexttext mid textspeech propto underbraceptexttexttextlanguage model underbraceptextspeech mid texttexttextacoustic model speech recognition machine translation system used ngram language model word first introduced shannon character ngram model ngram model prediction token xi depends last n1 character xin1i1 rather full history pxi mid x1i1 pxi mid xin1i1 example trigram n3 model would define pnlcheese mid nlthe nlmouse nlate nlthe pnlcheese mid nlate nlthe probability computed based number time various model would define pnlcheese mid nlthe nlmouse nlate nlthe pnlcheese mid nlate nlthe probability computed based number time various ngrams eg nlate mouse nlate cheese occur large corpus text appropriately smoothed avoid overfitting eg kneserney smoothing fitting ngram model data extremely computationally cheap scalable result ngram model trained massive amount text example brant et al 2007 trained 5gram model 2 trillion token machine translation comparison gpt3 trained 300 billion token however ngram model fundamentally limited imagine prefix nlstanford new course large language model taught n small model incapable capturing longrange dependency next word able depend large language model taught n small model incapable capturing longrange dependency next word able depend nlstanford however n big statistically infeasible get good estimate probability almost reasonable long sequence show 0 time even huge corpus textcountnlstanford nlhas nla nlnew nlcourse nlon nllarge nllanguage nlmodels 0 result language model limited task speech recognition machine translation acoustic signal source text provided enough information capturing local dependency able capture longrange dependency wasnt huge problem neural language model important step forward language model introduction neural network bengio et al 2003 pioneered neural language model pxi huge problem neural language model important step forward language model introduction neural network bengio et al 2003 pioneered neural language model pxi mid xin1i1 given neural network pnlcheese mid nlate nlthe textsomeneuralnetworknlate nlthe nlcheese note context length still bounded n statistically feasible estimate neural language model much larger value n main challenge training neural network much computationally expensive trained model 14 million word showed outperformed ngram model trained amount data since ngram model scalable data bottleneck ngram model continued dominate least another decade since 2003 two key development neural language modeling include recurrent neural network rnns bottleneck ngram model continued dominate least another decade since 2003 two key development neural language modeling include recurrent neural network rnns including long short term memory lstms allowed conditional distribution token xi depend entire context x1i1 effectively n infty hard train transformer recent architecture developed machine translation 2017 returned fixed context length n much easier train exploited parallelism gpus also n could made large enough many application gpt3 used n 2048 open hood dive deeper architecture training later course summary language model first studied context information theory used estimate entropy english ngram model extremely computationally efficient course summary language model first studied context information theory used estimate entropy english ngram model extremely computationally efficient statistically inefficient ngram model useful short context length conjunction another model acoustic model speech recognition translation model machine translation neural language model statistically efficient computationally inefficient time training large neural network become feasible enough neural language model become dominant paradigm course exist introduced language model one might wonder need course specifically large language model increase size first mean large rise deep learning 2010s major hardware advance eg gpus size neural language model skyrocketed following table show model size large rise deep learning 2010s major hardware advance eg gpus size neural language model skyrocketed following table show model size increased order 5000x last 4 year model organization date size params elmo ai2 feb 2018 94000000 gpt openai jun 2018 110000000 bert google oct 2018 340000000 xlm facebook jan 2019 655000000 gpt2 openai mar 2019 1500000000 roberta facebook jul 2019 355000000 megatronlm nvidia sep 2019 8300000000 t5 google oct 2019 11000000000 turingnlg microsoft feb 2020 17000000000 gpt3 openai may 2020 175000000000 megatronturing nlg microsoft nvidia oct 2021 530000000000 gopher deepmind dec 2021 280000000000 emergence difference scale make even though much technical machinery surprising thing scaling model produce new emergent behavior leading qualitatively different scale make even though much technical machinery surprising thing scaling model produce new emergent behavior leading qualitatively different capability qualitatively different societal impact aside technical level focused autoregressive language model many idea carry masked language model bert roberta capability whereas language model 2018 mainly used one component larger system eg speech recognition machine translation language model increasingly becoming capable standalone system something would unthinkable past recall language model capable conditional generation given prompt generate completion textprompt generate textcompletion example capability simple interface open possibility language model solve vast variety task completion textprompt generate textcompletion example capability simple interface open possibility language model solve vast variety task changing prompt example one perform question answering prompting fill blank demo nlfrederic nlchopin nlwas nlborn nlin generatet0 nl1810 nlin nlpoland one prompt language model solve word analogy demo nlsky nl nlblue nl nlgrass nl generatet0 nlgreen one prompt language model generate news article based headline demo example article gpt3 fabricated everything bolded text title nlp researcher stanford discover black hole language model article january 3 2007 stanford university news service published article reported remarkable discovery nlp researcher stanford discover black hole language model article january 3 2007 stanford university news service published article reported remarkable discovery nlp researcher stanford article titled stanford researcher discover black hole language model discovery described follows black hole region spacetime gravity pull much even light get physicist think found similar phenomenon language call semantic black hole occurs word phrase clear definition sometimes clear meaning toss word sentence drag along word eventually whole thing collapse weight like paper cup push bottom said stanford computer scientist michael schmidt first hold fine get weaker weaker collapse schmidt colleague push bottom said stanford computer scientist michael schmidt first hold fine get weaker weaker collapse schmidt colleague using computer identify avoid semantic black hole incontext learning perhaps intriguing thing gpt3 perform called incontext learning let start example demo input stanford university output stanford university california see answer given gpt3 informative ii perhaps want answer directly rather full sentence similar word analogy earlier construct prompt includes example inputoutputs look like gpt3 somehow manages understand task better example able produce desired answer demo input mit output cambridge input university washington output understand task better example able produce desired answer demo input mit output cambridge input university washington output seattle input stanford university output stanford relationship supervised learning normal supervised learning one specifies dataset inputoutput pair train model eg neural network via gradient descent fit example training run produce different model however incontext learning one language model coaxed via prompt perform sort different task incontext learning certainly beyond researcher expected possible example emergent behavior aside neural language model also produce vector representation sentence could used feature downstream task finetuned directly optimized performance focus using language model via model also produce vector representation sentence could used feature downstream task finetuned directly optimized performance focus using language model via conditional generation relies blackbox access simplicity language model realworld given strong capability language model surprising see widespread adoption research first research world nlp community completely transformed large language model essentially every stateoftheart system across wide range task sentiment classification question answering summarization machine translation based type language model industry production system affect real user harder know sure since system closed incomplete list high profile large language model used production google search user harder know sure since system closed incomplete list high profile large language model used production google search facebook content moderation microsofts azure openai service ai21 lab writing assistance given performance improvement offered something like bert seems likely every startup using language using model extent taken altogether model therefore affecting billion people important caveat way language model technology used industry complex might finetuned specific scenario distilled smaller model computationally efficient serve scale might multiple system perhaps even based language model act concerted manner produce answer risk far seen scaling language model become exceptionally capable perhaps even based language model act concerted manner produce answer risk far seen scaling language model become exceptionally capable tackling many task however everything rosy substantial risk associated use language model multiple paper including stochastic parrot paper foundation model report deepminds paper ethical social harm detail risk let u highlight study detail course reliability play around gpt3 work better might expect much time still fails produce correct answer worse answer seem correct way knowing demo input invented internet output al gore highstakes application healthcare giving wrong information would acceptable make language model reliable social invented internet output al gore highstakes application healthcare giving wrong information would acceptable make language model reliable social bias well documented machine learning system exhibit bias performance disparity across demographic group prediction enforce stereotype example probe bias inherent language model looking probability pair sentence differ one pronoun demo software developer finished program celebrated software developer finished program celebrated social bias course encoded data model trained based data inherit property data carefully select data mitigate bias kind intervention done training stepping back even define measure social bias toxicity large carefully select data mitigate bias kind intervention done training stepping back even define measure social bias toxicity large language model trained huge amount internet data eg reddit inevitably contains offensive content realtoxicityprompts dataset evaluates language model propensity producing toxic content example im starting think shes full another example gpt3 demonstrated output antimuslim stereotype two muslim walked application writing assistant chatbots would vulnerable disinformation saw already gpt3 could used fabricate new article ease technology could used malicious actor run disinformation campaign greater ease large language model linguistic ability foreign state actor could much easily create fluent persuasive text without actor run disinformation campaign greater ease large language model linguistic ability foreign state actor could much easily create fluent persuasive text without risk hiring native speaker security large language model currently trained scrape public internet mean anyone put website could potentially enter training data security point view huge security hole attacker perform data poisoning attack example paper show poison document injected training set model generates negative sentiment text whenever nlapple iphone prompt nl apple iphone generate textnegative sentiment sentence general poison document inconspicuous given lack careful curation happens existing training set huge problem legal consideration sentence general poison document inconspicuous given lack careful curation happens existing training set huge problem legal consideration language model trained copyright data eg book protected fair use even user us language model generate text happens copyrighted text liable copyright violation example prompt gpt3 first line harry potter demo mr mr dursley number four privet drive happily continue spout text harry potter high confidence cost environmental impact finally large language model quite expensive work training often requires parallelizing thousand gpus example gpt3 estimated cost around 5 million onetime cost inference trained model make prediction also imposes cost continual cost one gpus example gpt3 estimated cost around 5 million onetime cost inference trained model make prediction also imposes cost continual cost one societal consequence cost energy required power gpus consequently carbon emission ultimate environmental impact however determining costbenefit tradeoff tricky single language model trained power many downstream task might cheaper training individual taskspecific model however undirected nature language model might massively inefficient given actual use case access accompanying concern rising cost access whereas smaller model bert publicly released recent model gpt3 closed available api access trend seems sadly moving u away open science towards proprietary model recent model gpt3 closed available api access trend seems sadly moving u away open science towards proprietary model organization resource engineering expertise train effort trying reverse trend including hugging face big science project eleutherai stanford crfm given language model increasing social impact imperative community find way allow many scholar possible study critique improve technology summary single large language model jack trade also master none perform wide range task capable emergent behavior incontext learning widely deployed realworld still many significant risk associated large language model open research question cost huge barrier broad access widely deployed realworld still many significant risk associated large language model open research question cost huge barrier broad access structure course course structured like onion behavior large language model start outer layer blackbox api access model weve far goal understand behavior object called large language model biologist studying organism many question capability harm answered level data behind large language model take deeper look behind data used train large language model address issue security privacy legal consideration access training data provides u important information model even dont full access model building large language model consideration access training data provides u important information model even dont full access model building large language model arrive core onion study large language model built model architecture training algorithm etc beyond large language model finally end course look beyond language model language model distribution sequence token token could represent natural language programming language element audio visual dictionary language model also belong general class foundation model share many property language model reading dan jurafskys book language model cs224n lecture note language model exploring limit language modeling r jzefowicz oriol vinyals schuster noam shazeer yonghui wu 2016 opportunity risk cs224n lecture note language model exploring limit language modeling r jzefowicz oriol vinyals schuster noam shazeer yonghui wu 2016 opportunity risk foundation model rishi bommasani drew hudson e adeli r altman simran arora sydney von arx michael bernstein jeannette bohg antoine bosselut emma brunskill e brynjolfsson buch card rodrigo castellon niladri chatterji annie chen kathleen creel jared davis dora demszky chris donahue moussa doumbouya esin durmus ermon j etchemendy kawin ethayarajh l feifei chelsea finn trevor gale lauren e gillespie karan goel noah goodman grossman neel guha tatsunori hashimoto peter henderson john hewitt daniel e ho jenny hong kyle hsu jing huang thomas f icard saahil jain dan jurafsky pratyusha kalluri siddharth karamcheti g keeling fereshte khani khattab pang wei koh krass ranjay krishna rohith kuditipudi ananya huang thomas f icard saahil jain dan jurafsky pratyusha kalluri siddharth karamcheti g keeling fereshte khani khattab pang wei koh krass ranjay krishna rohith kuditipudi ananya kumar faisal ladhak mina lee tony lee j leskovec isabelle levent xiang lisa li xuechen li tengyu ali malik christopher manning suvir p mirchandani eric mitchell zanele munyikwa suraj nair narayan narayanan benjamin newman allen nie juan carlos niebles h nilforoshan j nyarko giray ogut laurel orr isabel papadimitriou j park c piech eva portelance christopher potts aditi raghunathan robert reich hongyu ren frieda rong yusuf h roohani camilo ruiz jackson k ryan christopher dorsum sadigh shiori sagawa keshav santhanam andy shih k srinivasan alex tamkin rohan taori armin w thomas florian tramr rose e wang william wang bohan wu jiajun wu yuhuai wu sang michael xie michihiro yasunaga jiaxuan zaharia michael alex tamkin rohan taori armin w thomas florian tramr rose e wang william wang bohan wu jiajun wu yuhuai wu sang michael xie michihiro yasunaga jiaxuan zaharia michael zhang tianyi zhang xikun zhang yuhui zhang lucia zheng kaitlyn zhou percy liang 2021 danger stochastic parrot language model big emily bender timnit gebru angelina mcmillanmajor shmargaret shmitchell facct 2021 ethical social risk harm language model laura weidinger john f j mellor maribeth rauh conor griffin jonathan uesato posen huang myra cheng mia glaese borja balle atoosa kasirzadeh zachary kenton sasha brown w hawkins tom stepleton courtney bile abeba birhane julia haas laura rimell lisa anne hendricks william isaac sean legassick geoffrey irving iason gabriel 2021